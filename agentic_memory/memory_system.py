import logging
import uuid
import json
import pickle  # <--- [æ–°å¢] ç”¨äºå¯¹è±¡åºåˆ—åŒ–
import os  # <--- [æ–°å¢] ç”¨äºè·¯å¾„æ“ä½œ
import shutil  # <--- [æ–°å¢] ç”¨äºæ–‡ä»¶å¤¹å¤åˆ¶
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime

# å¼•å…¥æˆ‘ä»¬é€‚é…å¥½çš„ç»„ä»¶
from .llm_controller import LLMController
from .retrievers import ChromaRetriever

logger = logging.getLogger(__name__)


class MemoryNote:
    """
    è®°å¿†èŠ‚ç‚¹ç±»ï¼šä»£è¡¨ç³»ç»Ÿä¸­çš„ä¸€ä¸ªæœ€å°ä¿¡æ¯å•å…ƒã€‚
    åŒ…å«äº†å†…å®¹ã€å…ƒæ•°æ®ä»¥åŠä¸å…¶ä»–è®°å¿†çš„é“¾æ¥å…³ç³»ã€‚
    """

    def __init__(self,
                 content: str,
                 id: Optional[str] = None,
                 keywords: Optional[List[str]] = None,
                 links: Optional[Dict] = None,
                 retrieval_count: Optional[int] = None,
                 timestamp: Optional[str] = None,
                 last_accessed: Optional[str] = None,
                 context: Optional[str] = None,
                 evolution_history: Optional[List] = None,
                 category: Optional[str] = None,
                 tags: Optional[List[str]] = None):
        # æ ¸å¿ƒå†…å®¹ä¸ ID
        self.content = content
        self.id = id or str(uuid.uuid4())

        # è¯­ä¹‰å…ƒæ•°æ®
        self.keywords = keywords or []
        self.links = links or []  # [å…³é”®] è¿™é‡Œå­˜å‚¨äº†å›¾è°±ç»“æ„ï¼ˆLink åˆ°å…¶ä»–è®°å¿†çš„ IDï¼‰
        self.context = context or "General"
        self.category = category or "Uncategorized"
        self.tags = tags or []

        # æ—¶é—´ä¿¡æ¯
        current_time = datetime.now().strftime("%Y%m%d%H%M")
        self.timestamp = timestamp or current_time
        self.last_accessed = last_accessed or current_time

        # ç»Ÿè®¡ä¸è¿›åŒ–å†å²
        self.retrieval_count = retrieval_count or 0
        self.evolution_history = evolution_history or []


class AgenticMemorySystem:
    """
    A-mem æ ¸å¿ƒç³»ç»Ÿï¼šæ”¯æŒ API é©±åŠ¨ï¼Œå…·å¤‡å®Œæ•´çš„ CRUD å’Œè¿›åŒ–èƒ½åŠ›ã€‚
    """

    def __init__(self,
                 model_name: str = 'all-MiniLM-L6-v2',  # ä¿ç•™å‚æ•°åä»¥å…¼å®¹æ—§é…ç½®ï¼Œå®é™… API æ¨¡å¼ä¸‹ç”± embedding_config æ§åˆ¶
                 llm_backend: str = "openai",
                 llm_model: str = "gpt-4o-mini",
                 llm_api_key: Optional[str] = None,
                 llm_base_url: Optional[str] = None,
                 embedding_config: Optional[Dict] = None,
                 evo_threshold: int = 100,
                 enable_evolution: bool = True,
                 persist_dir: Optional[str] = None):  # <--- [æ–°å¢] æ¥æ”¶æŒä¹…åŒ–ç›®å½•å‚æ•°

        self.memories = {}
        self.model_name = model_name
        self.embedding_config = embedding_config
        self.enable_evolution = enable_evolution
        self.persist_dir = persist_dir  # <--- [æ–°å¢] ä¿å­˜ç›®å½•è·¯å¾„

        # === [æ–°å¢] è·¯å¾„å‡†å¤‡ ===
        chroma_path = None
        if self.persist_dir:
            # å¦‚æœæŒ‡å®šäº†ç›®å½•ï¼Œè‡ªåŠ¨åˆ›å»º
            if not os.path.exists(self.persist_dir):
                os.makedirs(self.persist_dir)
            chroma_path = os.path.join(self.persist_dir, "chroma_db")
        # ======================

        # 1. åˆå§‹åŒ– ChromaDB
        try:
            # === [ä¿®æ”¹] ä»…åœ¨æ²¡æœ‰æŒä¹…åŒ–éœ€æ±‚æ—¶æ‰å¼ºåˆ¶é‡ç½®ï¼Œå¦åˆ™æˆ‘ä»¬å¸Œæœ›åŠ è½½æ—§æ•°æ® ===
            if not self.persist_dir:
                temp_retriever = ChromaRetriever(collection_name="memories",
                                                 embedding_config=self.embedding_config)
                temp_retriever.client.reset()  # çº¯å†…å­˜æ¨¡å¼å¯åŠ¨æ—¶é‡ç½®
        except Exception as e:
            logger.warning(f"Could not reset ChromaDB collection: {e}")

        # === [ä¿®æ”¹] ä¼ å…¥ persist_path ç»™ Retriever ===
        self.retriever = ChromaRetriever(collection_name="memories",
                                         embedding_config=self.embedding_config,
                                         persist_path=chroma_path)

        # 2. åˆå§‹åŒ– LLM
        self.llm_controller = LLMController(
            backend=llm_backend,
            model=llm_model,
            api_key=llm_api_key,
            base_url=llm_base_url
        )

        self.evo_cnt = 0
        self.evo_threshold = evo_threshold

        # === [æ–°å¢] å°è¯•ä»ç¡¬ç›˜åŠ è½½æ—§çš„è®°å¿†å¯¹è±¡ ===
        if self.persist_dir:
            self.load_state()
        # ======================================

        # 3. å®Œæ•´çš„è¿›åŒ– Prompt
        self._evolution_system_prompt = '''
You are an AI memory evolution agent responsible for managing and evolving a knowledge base.
Analyze the the new memory note according to keywords and context, also with their several nearest neighbors memory.
Make decisions about its evolution.  

The new memory context:
{context}
content: {content}
keywords: {keywords}

The nearest neighbors memories:
{nearest_neighbors_memories}

Based on this information, determine:
1. Should this memory be evolved? Consider its relationships with other memories.
2. What specific actions should be taken (strengthen, update_neighbor)?
   2.1 If choose to strengthen the connection, which memory should it be connected to? Can you give the updated tags of this memory?
   2.2 If choose to update_neighbor, you can update the context and tags of these memories based on the understanding of these memories. If the context and the tags are not updated, the new context and tags should be the same as the original ones. Generate the new context and tags in the sequential order of the input neighbors.
Tags should be determined by the content of these characteristic of these memories, which can be used to retrieve them later and categorize them.
Note that the length of new_tags_neighborhood must equal the number of input neighbors, and the length of new_context_neighborhood must equal the number of input neighbors.
The number of neighbors is {neighbor_number}.

Return your decision in JSON format with the following structure:
{{
    "should_evolve": true,
    "actions": ["strengthen", "update_neighbor"],
    "suggested_connections": ["neighbor_memory_id_1", "neighbor_memory_id_2"],
    "tags_to_update": ["tag_1", "tag_n"], 
    "new_context_neighborhood": ["new context 1", "new context 2"],
    "new_tags_neighborhood": [["tag_1", "tag_n"], ["tag_1", "tag_n"]]
}}
'''

    # === [æ–°å¢] ä¿å­˜çŠ¶æ€åˆ°ç£ç›˜ ===
    def save_state(self):
        """å°†å†…å­˜ä¸­çš„ memories å­—å…¸ä¿å­˜åˆ° pickle æ–‡ä»¶"""
        if not self.persist_dir:
            return

        pkl_path = os.path.join(self.persist_dir, "memories.pkl")
        try:
            with open(pkl_path, 'wb') as f:
                pickle.dump(self.memories, f)
            # logger.info(f"Memory state saved to {pkl_path}") # å¯é€‰ï¼šæ‰“å°æ—¥å¿—
        except Exception as e:
            logger.error(f"Failed to save memory state: {e}")

    # === [æ–°å¢] ä»ç£ç›˜åŠ è½½çŠ¶æ€ ===
    def load_state(self):
        """ä» pickle æ–‡ä»¶åŠ è½½ memories å­—å…¸"""
        if not self.persist_dir:
            return

        pkl_path = os.path.join(self.persist_dir, "memories.pkl")
        if os.path.exists(pkl_path):
            try:
                with open(pkl_path, 'rb') as f:
                    self.memories = pickle.load(f)
                logger.info(f"Loaded {len(self.memories)} memories from {pkl_path}")
            except Exception as e:
                logger.error(f"Failed to load memory state: {e}")

    # === [æ–°å¢] åˆ›å»ºå¿«ç…§åŠŸèƒ½ (æ ¸å¿ƒä¿®æ”¹ç‚¹) ===
    def create_snapshot(self, snapshot_path: str):
        """
        å°†å½“å‰çš„æŒä¹…åŒ–ç›®å½• (persist_dir) å®Œæ•´å¤åˆ¶åˆ° snapshot_pathã€‚
        è¿™åŒ…æ‹¬ memories.pkl å’Œ chroma_db æ–‡ä»¶å¤¹ã€‚
        """
        if not self.persist_dir:
            logger.warning("Cannot snapshot: No persist_dir configured.")
            return

        # 1. å¼ºåˆ¶ä¿å­˜ä¸€æ¬¡æœ€æ–°çš„å†…å­˜çŠ¶æ€ï¼Œç¡®ä¿ pkl æ˜¯æœ€æ–°çš„
        self.save_state()

        # 2. ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨ï¼ˆå¦‚æœçˆ¶ç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰
        if not os.path.exists(snapshot_path):
            os.makedirs(snapshot_path, exist_ok=True)

        try:
            # 3. å¤åˆ¶ memories.pkl (å¯¹è±¡å›¾è°±)
            src_pkl = os.path.join(self.persist_dir, "memories.pkl")
            if os.path.exists(src_pkl):
                shutil.copy2(src_pkl, os.path.join(snapshot_path, "memories.pkl"))

            # 4. å¤åˆ¶ chroma_db æ–‡ä»¶å¤¹ (å‘é‡ç´¢å¼•)
            src_chroma = os.path.join(self.persist_dir, "chroma_db")
            dst_chroma = os.path.join(snapshot_path, "chroma_db")

            # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼ˆæå°‘æƒ…å†µï¼‰ï¼Œå…ˆåˆ é™¤ï¼Œé¿å… copytree æŠ¥é”™
            if os.path.exists(dst_chroma):
                shutil.rmtree(dst_chroma)

            if os.path.exists(src_chroma):
                # ignore_errors=True å¯ä»¥å¿½ç•¥ Windows ä¸‹æŸäº›ä¸´æ—¶æ–‡ä»¶è¢«é”å®šçš„é”™è¯¯
                shutil.copytree(src_chroma, dst_chroma, ignore=shutil.ignore_patterns('*.lock'))

            logger.info(f"Snapshot created successfully at: {snapshot_path}")

        except Exception as e:
            logger.error(f"Failed to create snapshot: {e}")

    # ============================

    def analyze_content(self, content: str) -> Dict:
        """
        è°ƒç”¨ LLM åˆ†æå†…å®¹ï¼Œæå–å…ƒæ•°æ®ã€‚
        è¿™æ˜¯è®°å¿†ç”Ÿæˆçš„å¿…è¦æ­¥éª¤ã€‚
        """
        prompt = """Generate a structured analysis of the following content by:
            1. Identifying the most salient keywords (focus on nouns, verbs, and key concepts)
            2. Extracting core themes and contextual elements
            3. Creating relevant categorical tags

            Format the response as a JSON object:
            {
                "keywords": ["keyword1", "keyword2"],
                "context": "One sentence summary",
                "tags": ["tag1", "tag2"]
            }

            Content for analysis:
            """ + content

        response_schema = {"type": "json_object"}
        try:
            response = self.llm_controller.llm.get_completion(prompt, response_format=response_schema)
            cleaned = response.replace("```json", "").replace("```", "").strip()
            return json.loads(cleaned)
        except Exception as e:
            logger.error(f"Error analyzing content: {e}")
            return {"keywords": [], "context": "General", "tags": []}

    def add_note(self, content: str, time: str = None, **kwargs) -> str:
        """
        [æ ¸å¿ƒå…¥å£] æ·»åŠ ç¬”è®° -> (å¯é€‰è¿›åŒ–) -> å­˜å…¥
        """
        if time is not None:
            kwargs['timestamp'] = time

        # 1. è‡ªåŠ¨è¡¥å…¨å…ƒæ•°æ®
        if not kwargs.get('keywords') or not kwargs.get('context'):
            analysis = self.analyze_content(content)
            kwargs['keywords'] = kwargs.get('keywords') or analysis.get('keywords', [])
            kwargs['context'] = kwargs.get('context') or analysis.get('context', "General")
            kwargs['tags'] = kwargs.get('tags') or analysis.get('tags', [])

        note = MemoryNote(content=content, **kwargs)

        # 2. è¿›åŒ– (Evolution) - ç”±å¼€å…³æ§åˆ¶
        evo_label = False
        if self.enable_evolution:
            # è°ƒç”¨æ ¸å¿ƒè¿›åŒ–é€»è¾‘
            evo_label, note = self.process_memory(note)

        self.memories[note.id] = note

        # 3. åºåˆ—åŒ–å¹¶å­˜å…¥ ChromaDB
        # æ³¨æ„ï¼šChromaDB å…ƒæ•°æ®ä¸æ”¯æŒåˆ—è¡¨ï¼Œå¿…é¡» json.dumps è½¬å­—ç¬¦ä¸²
        metadata = {
            "id": note.id,
            "content": note.content,
            "keywords": json.dumps(note.keywords),
            "links": json.dumps(note.links),
            "retrieval_count": note.retrieval_count,
            "timestamp": note.timestamp,
            "last_accessed": note.last_accessed,
            "context": note.context,
            "evolution_history": str(note.evolution_history),
            "category": note.category,
            "tags": json.dumps(note.tags)
        }
        self.retriever.add_document(note.content, metadata, note.id)

        # è§¦å‘å®šæœŸæ•´ç† (åŸç‰ˆé€»è¾‘)
        if evo_label == True:
            self.evo_cnt += 1
            if self.evo_cnt % self.evo_threshold == 0:
                self.consolidate_memories()

        # === [æ–°å¢] æ¯æ¬¡æ·»åŠ åè‡ªåŠ¨ä¿å­˜åˆ°ç£ç›˜ ===
        if self.persist_dir:
            self.save_state()
        # ====================================

        return note.id

    def consolidate_memories(self):
        """é‡å»ºç´¢å¼• (ç»´æŠ¤ç”¨)"""
        # === [ä¿®æ”¹] é‡å»ºç´¢å¼•æ—¶ä¹Ÿéœ€è¦ä¿æŒæŒä¹…åŒ–è·¯å¾„ ===
        chroma_path = os.path.join(self.persist_dir, "chroma_db") if self.persist_dir else None

        self.retriever = ChromaRetriever(collection_name="memories",
                                         embedding_config=self.embedding_config,
                                         persist_path=chroma_path)  # <--- [ä¿®æ”¹] ä¼ å…¥è·¯å¾„

        for memory in self.memories.values():
            metadata = {
                "id": memory.id, "content": memory.content, "keywords": json.dumps(memory.keywords),
                "links": json.dumps(memory.links), "retrieval_count": memory.retrieval_count,
                "timestamp": memory.timestamp, "last_accessed": memory.last_accessed,
                "context": memory.context, "evolution_history": str(memory.evolution_history),
                "category": memory.category, "tags": json.dumps(memory.tags)
            }
            self.retriever.add_document(memory.content, metadata, memory.id)

        # === [æ–°å¢] æ•´ç†å®Œåä¿å­˜ä¸€æ¬¡ ===
        if self.persist_dir:
            self.save_state()

    def find_related_memories(self, query: str, k: int = 5) -> Tuple[str, List[str]]:
        """
        [è¿›åŒ–è¾…åŠ©] æŸ¥æ‰¾ç›¸å…³è®°å¿†å¹¶è¿”å›æ ¼å¼åŒ–å­—ç¬¦ä¸²ã€‚
        æ­¤å‡½æ•°è¢« process_memory è°ƒç”¨ï¼Œç”¨äºç»™ LLM æä¾›ä¸Šä¸‹æ–‡ã€‚
        """
        if not self.memories:
            return "", []
        try:
            results = self.retriever.search(query, k)
            memory_str = ""
            found_ids = []
            if 'ids' in results and results['ids'] and len(results['ids']) > 0:
                for i, doc_id in enumerate(results['ids'][0]):
                    if doc_id in self.memories:
                        mem = self.memories[doc_id]
                        # æ ¼å¼åŒ–ä¸ºæ–‡æœ¬ä¾› LLM é˜…è¯»
                        memory_str += f"memory id:{doc_id}\tcontent: {mem.content}\tcontext: {mem.context}\tkeywords: {mem.keywords}\ttags: {mem.tags}\n"
                        found_ids.append(doc_id)
            return memory_str, found_ids
        except Exception as e:
            logger.error(f"Error in find_related_memories: {str(e)}")
            return "", []

    def find_related_memories_raw(self, query: str, k: int = 5) -> str:
        """[å®Œæ•´æ€§ä¿ç•™] è¿”å› raw æ ¼å¼å­—ç¬¦ä¸²"""
        return self.find_related_memories(query, k)[0]

    def read(self, memory_id: str) -> Optional[MemoryNote]:
        """[å®Œæ•´æ€§ä¿ç•™] è¯»å–å•æ¡è®°å¿†"""
        return self.memories.get(memory_id)

    def update(self, memory_id: str, **kwargs) -> bool:
        """[å®Œæ•´æ€§ä¿ç•™] æ›´æ–°è®°å¿†å†…å®¹"""
        if memory_id not in self.memories:
            return False
        note = self.memories[memory_id]
        for key, value in kwargs.items():
            if hasattr(note, key):
                setattr(note, key, value)

        metadata = {
            "id": note.id, "content": note.content, "keywords": json.dumps(note.keywords),
            "links": json.dumps(note.links), "retrieval_count": note.retrieval_count,
            "timestamp": note.timestamp, "last_accessed": note.last_accessed,
            "context": note.context, "evolution_history": str(note.evolution_history),
            "category": note.category, "tags": json.dumps(note.tags)
        }
        self.retriever.delete_document(memory_id)
        self.retriever.add_document(document=note.content, metadata=metadata, doc_id=memory_id)

        # === [æ–°å¢] æ›´æ–°åè‡ªåŠ¨ä¿å­˜ ===
        if self.persist_dir:
            self.save_state()

        return True

    def delete(self, memory_id: str) -> bool:
        """[å®Œæ•´æ€§ä¿ç•™] åˆ é™¤è®°å¿†"""
        if memory_id in self.memories:
            self.retriever.delete_document(memory_id)
            del self.memories[memory_id]

            # === [æ–°å¢] åˆ é™¤åè‡ªåŠ¨ä¿å­˜ ===
            if self.persist_dir:
                self.save_state()

            return True
        return False

    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """[å®Œæ•´æ€§ä¿ç•™] åŸºç¡€å‘é‡æœç´¢"""
        results = self.retriever.search(query, k)
        memories = []
        if not results or 'ids' not in results: return []
        for i, doc_id in enumerate(results['ids'][0]):
            mem = self.memories.get(doc_id)
            if mem:
                memories.append({
                    'id': doc_id,
                    'content': mem.content,
                    'context': mem.context,
                    'keywords': mem.keywords,
                    'score': results['distances'][0][i] if 'distances' in results else 0
                })
        return memories

    def search_agentic(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        [ä¿®å¤ç‰ˆ] å‘é‡æ£€ç´¢ + å›¾è°±æ‰©å±•
        ä¿®å¤äº†â€œé‚»å±…è¢«æˆªæ–­â€çš„ Bugã€‚ç°åœ¨è¿”å›ç»“æœæ•°é‡å¯èƒ½ä¼šè¶…è¿‡ kã€‚
        """
        if not self.memories:
            return []

        try:
            # 1. å‘é‡æ£€ç´¢ (Base Retrieval)
            results = self.retriever.search(query, k)
            memories = []
            seen_ids = set()

            if not results or 'ids' not in results or not results['ids']:
                return []

            # 2. å¤„ç†å‘é‡ç»“æœ
            for i, doc_id in enumerate(results['ids'][0]):
                if doc_id in seen_ids: continue

                mem_obj = self.memories.get(doc_id)
                if mem_obj:
                    content = mem_obj.content
                    context = mem_obj.context
                    keywords = mem_obj.keywords
                    tags = mem_obj.tags
                    links = mem_obj.links
                else:
                    meta = results['metadatas'][0][i]
                    content = meta.get('content', '')
                    context = meta.get('context', '')
                    keywords = meta.get('keywords', [])
                    tags = meta.get('tags', [])
                    links = meta.get('links', [])

                memories.append({
                    'id': doc_id,
                    'content': content,
                    'context': context,
                    'keywords': keywords,
                    'tags': tags,
                    'links': links,
                    'is_neighbor': False
                })
                seen_ids.add(doc_id)

            # 3. å›¾è°±æ‰©å±• (Graph Expansion)
            # æˆ‘ä»¬å…è®¸åœ¨ k çš„åŸºç¡€ä¸Šï¼Œé¢å¤–æ‰©å±•å‡ºä¸€äº›é‚»å±…
            # æ¯”å¦‚å…è®¸æ¯ä¸ªå‘é‡ç»“æœå¸¦å‡ºå®ƒçš„æ‰€æœ‰ä¸€çº§è¿æ¥

            # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬è¿›è¡Œéå†ï¼Œé˜²æ­¢åœ¨å¾ªç¯ä¸­ä¿®æ”¹åˆ—è¡¨å¯¼è‡´çš„é—®é¢˜
            base_memories = list(memories)

            for memory in base_memories:
                links = memory.get('links', [])
                if isinstance(links, str):
                    try:
                        links = json.loads(links)
                    except:
                        links = []

                for link_id in links:
                    # åªè¦æ²¡è§è¿‡ï¼Œå°±åŠ è¿›æ¥ï¼ä¸è¦å— k çš„é™åˆ¶ï¼
                    if link_id not in seen_ids:
                        neighbor = self.memories.get(link_id)
                        if neighbor:
                            memories.append({
                                'id': link_id,
                                'content': neighbor.content,
                                'context': neighbor.context,
                                'keywords': neighbor.keywords,
                                'tags': neighbor.tags,
                                'is_neighbor': True
                            })
                            seen_ids.add(link_id)

            # ğŸ”´ æœ€ç»ˆè¿”å›æ‰€æœ‰ç»“æœ (å‘é‡ + é‚»å±…)
            # ä¸è¦ [:k]ï¼Œå¦åˆ™è¾›è‹¦æ‰¾æ¥çš„é‚»å±…å…¨æ²¡äº†
            return memories

        except Exception as e:
            logger.error(f"Error in search_agentic: {str(e)}")
            return []

    def process_memory(self, note: MemoryNote) -> Tuple[bool, MemoryNote]:
        """
        [è¿›åŒ–æ ¸å¿ƒ] å¤„ç†è®°å¿†è¿›åŒ–
        è°ƒç”¨ LLM åˆ¤æ–­æ–°è®°å¿†æ˜¯å¦åº”è¯¥ä¸æ—§è®°å¿†å»ºç«‹è¿æ¥ (Link) æˆ–æ›´æ–°æ ‡ç­¾ã€‚
        """
        if not self.memories:
            return False, note
        try:
            # 1. å¯»æ‰¾æ½œåœ¨çš„å…³è”å¯¹è±¡ (Vector Top-10)
            neighbors_text, neighbor_ids = self.find_related_memories(note.content, k=10)
            if not neighbors_text:
                return False, note

            # 2. æ„é€  Prompt
            prompt = self._evolution_system_prompt.format(
                content=note.content,
                context=note.context,
                keywords=str(note.keywords),
                nearest_neighbors_memories=neighbors_text,
                neighbor_number=len(neighbor_ids)
            )

            response_schema = {"type": "json_object"}
            try:
                # 3. è°ƒç”¨ LLM å†³ç­–
                response = self.llm_controller.llm.get_completion(
                    prompt,
                    response_format=response_schema
                )
                cleaned = response.replace("```json", "").replace("```", "").strip()
                response_json = json.loads(cleaned)
                should_evolve = response_json.get("should_evolve", False)

                # 4. æ‰§è¡Œè¿›åŒ–åŠ¨ä½œ
                if should_evolve:
                    actions = response_json.get("actions", [])
                    for action in actions:
                        if action == "strengthen":
                            # å»ºç«‹è¿æ¥
                            suggested = response_json.get("suggested_connections", [])
                            valid_links = [nid for nid in suggested if nid in self.memories]

                            # 1. æ­£å‘è¿æ¥ï¼šæ–°è®°å¿† -> æ—§è®°å¿†
                            if not isinstance(note.links, list): note.links = []
                            note.links.extend(valid_links)

                            # 2. ã€æ–°å¢ã€‘åå‘è¿æ¥ï¼šæ—§è®°å¿† -> æ–°è®°å¿†
                            # å¿…é¡»æŠŠå½“å‰ note.id åŠ åˆ°é‚£äº›è¢«å¼•ç”¨çš„æ—§è®°å¿†çš„ links é‡Œ
                            for nid in valid_links:
                                neighbor = self.memories[nid]
                                if not isinstance(neighbor.links, list): neighbor.links = []
                                # é¿å…é‡å¤æ·»åŠ 
                                if note.id not in neighbor.links:
                                    neighbor.links.append(note.id)
                                    # è¿™ä¸€æ­¥å¾ˆé‡è¦ï¼šå› ä¸ºæˆ‘ä»¬ä¿®æ”¹äº†æ—§è®°å¿†ï¼Œéœ€è¦æ›´æ–° ChromaDB é‡Œçš„å…ƒæ•°æ®
                                    # ä½†ä¸ºäº†æ€§èƒ½ï¼Œè¿™é‡Œå¯ä»¥åªåœ¨å†…å­˜æ”¹ï¼Œæœ€åç»Ÿä¸€ consolidate
                                    # æˆ–è€…åœ¨è¿™é‡Œæ˜¾å¼è°ƒç”¨ update (ä¼šæ…¢ä¸€ç‚¹)
                                    # self.update(nid, links=neighbor.links)

                            # æ›´æ–°æ ‡ç­¾
                            new_tags = response_json.get("tags_to_update", [])
                            if new_tags: note.tags = new_tags

                        elif action == "update_neighbor":
                            # åå‘æ›´æ–°é‚»å±…è®°å¿†çš„ä¸Šä¸‹æ–‡
                            new_ctxs = response_json.get("new_context_neighborhood", [])
                            new_tags_list = response_json.get("new_tags_neighborhood", [])

                            for idx, nid in enumerate(neighbor_ids):
                                if nid in self.memories:
                                    neighbor_mem = self.memories[nid]
                                    if idx < len(new_ctxs):
                                        neighbor_mem.context = new_ctxs[idx]
                                    if idx < len(new_tags_list):
                                        neighbor_mem.tags = new_tags_list[idx]

                return should_evolve, note
            except Exception as e:
                logger.error(f"Error in evolution execution: {e}")
                return False, note
        except Exception as e:
            logger.error(f"Error in process_memory: {str(e)}")
            return False, note